{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for the number of images in a folder\n",
    "import os\n",
    "food_image_paths = []\n",
    "for dir, sub_dir, files in os.walk(\"data_small_test\\\\train\"):\n",
    "    for file in files:\n",
    "        food_image_paths.append(os.path.join(dir,file))\n",
    "len(food_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data_small_test\\\\train', 'data_small_test\\\\test')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = \"data_small_test\\\\train\"\n",
    "test_dir = \"data_small_test\\\\test\"\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 files belonging to 2 classes.\n",
      "Found 400 files belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in data\n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
    "            batch_size=32,\n",
    "            image_size=(224,224)\n",
    "            )\n",
    "\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "            batch_size=32,\n",
    "            image_size=(224,224)\n",
    "            )\n",
    "\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_pie', 'baby_back_ribs']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
    "test_data = test_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "# Make model untrainable\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build a functional model\n",
    "input_layer = tf.keras.Input(shape=(224,224,3))\n",
    "x = base_model(input_layer)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Construct model\n",
    "model_1 = tf.keras.Model(input_layer, output_layer, name=\"EfficientNetB0-V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12272/463032860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\sutyl\\anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m   \u001b[1;31m# Save image to disk.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m   \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextension\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m   \u001b[1;31m# Return the image as a Jupyter Image object, to be displayed in-line.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m   \u001b[1;31m# Note that we cannot easily detect whether the code is running in a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\sutyl\\anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, path, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1827\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1828\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1829\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1830\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1831\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.png'"
     ]
    }
   ],
   "source": [
    "# tf.keras.utils.plot_model(model_1, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                 optimizer= tf.keras.optimizers.Adam(),\n",
    "                 metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EarlyStopping callback and TensorBoard callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5,\n",
    "                                                  monitor=\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50/50 [==============================] - 93s 2s/step - loss: 0.3887 - accuracy: 0.8900 - val_loss: 0.2210 - val_accuracy: 0.9675\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.1772 - accuracy: 0.9694 - val_loss: 0.1425 - val_accuracy: 0.9775\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 83s 2s/step - loss: 0.1276 - accuracy: 0.9762 - val_loss: 0.1115 - val_accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "history_1 = model_1.fit(train_data,\n",
    "                        epochs=3,\n",
    "                        validation_data=test_data,\n",
    "                        callbacks=[early_stopping]\n",
    "                        ) # Adjust for imbalance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 17s 1s/step - loss: 0.1115 - accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1114860251545906, 0.9800000190734863]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and convert model to tfjs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight normalization/count with shape () and dtype int64 was auto converted to the type int32\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Failed to create a NewWriteableFile: model\\tfjs_model\\group1-shard1of4.bin : The system cannot find the file specified.\r\n; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32md:\\Programming\\Code Workplace\\Python\\MyProjects\\Food_not_food\\small_model_building.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming/Code%20Workplace/Python/MyProjects/Food_not_food/small_model_building.ipynb#ch0000008?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming/Code%20Workplace/Python/MyProjects/Food_not_food/small_model_building.ipynb#ch0000008?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mload_model(\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mfood_not_food_model_B0-V1.h5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Programming/Code%20Workplace/Python/MyProjects/Food_not_food/small_model_building.ipynb#ch0000008?line=7'>8</a>\u001b[0m tfjs\u001b[39m.\u001b[39;49mconverters\u001b[39m.\u001b[39;49msave_keras_model(model, \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mtfjs_model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sutyl\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflowjs\\converters\\keras_h5_conversion.py:354\u001b[0m, in \u001b[0;36msave_keras_model\u001b[1;34m(model, artifacts_dir, quantization_dtype_map, weight_shard_size_bytes, metadata)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(artifacts_dir):\n\u001b[0;32m    353\u001b[0m   os\u001b[39m.\u001b[39mmakedirs(artifacts_dir)\n\u001b[1;32m--> 354\u001b[0m write_artifacts(\n\u001b[0;32m    355\u001b[0m     topology_json, weight_groups, artifacts_dir,\n\u001b[0;32m    356\u001b[0m     quantization_dtype_map\u001b[39m=\u001b[39;49mquantization_dtype_map,\n\u001b[0;32m    357\u001b[0m     weight_shard_size_bytes\u001b[39m=\u001b[39;49mweight_shard_size_bytes,\n\u001b[0;32m    358\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata)\n\u001b[0;32m    359\u001b[0m os\u001b[39m.\u001b[39mremove(temp_h5_path)\n",
      "File \u001b[1;32mc:\\Users\\sutyl\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflowjs\\converters\\keras_h5_conversion.py:305\u001b[0m, in \u001b[0;36mwrite_artifacts\u001b[1;34m(topology, weights, output_dir, quantization_dtype_map, weight_shard_size_bytes, metadata)\u001b[0m\n\u001b[0;32m    302\u001b[0m   model_json[common\u001b[39m.\u001b[39mUSER_DEFINED_METADATA_KEY] \u001b[39m=\u001b[39m metadata\n\u001b[0;32m    304\u001b[0m model_json[common\u001b[39m.\u001b[39mARTIFACT_MODEL_TOPOLOGY_KEY] \u001b[39m=\u001b[39m topology \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m weights_manifest \u001b[39m=\u001b[39m write_weights\u001b[39m.\u001b[39;49mwrite_weights(\n\u001b[0;32m    306\u001b[0m     weights, output_dir, write_manifest\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    307\u001b[0m     quantization_dtype_map\u001b[39m=\u001b[39;49mquantization_dtype_map,\n\u001b[0;32m    308\u001b[0m     shard_size_bytes\u001b[39m=\u001b[39;49mweight_shard_size_bytes)\n\u001b[0;32m    309\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(weights_manifest, \u001b[39mlist\u001b[39m)\n\u001b[0;32m    310\u001b[0m model_json[common\u001b[39m.\u001b[39mARTIFACT_WEIGHTS_MANIFEST_KEY] \u001b[39m=\u001b[39m weights_manifest\n",
      "File \u001b[1;32mc:\\Users\\sutyl\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflowjs\\write_weights.py:135\u001b[0m, in \u001b[0;36mwrite_weights\u001b[1;34m(weight_groups, write_dir, shard_size_bytes, write_manifest, quantization_dtype_map)\u001b[0m\n\u001b[0;32m    129\u001b[0m group \u001b[39m=\u001b[39m [\n\u001b[0;32m    130\u001b[0m     _quantize_entry(e, quantization_dtype[e[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[0;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m e[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m] \u001b[39min\u001b[39;00m quantization_dtype \u001b[39melse\u001b[39;00m e \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m group\n\u001b[0;32m    132\u001b[0m ]\n\u001b[0;32m    133\u001b[0m group_bytes, total_bytes, _ \u001b[39m=\u001b[39m _stack_group_bytes(group)\n\u001b[1;32m--> 135\u001b[0m shard_filenames \u001b[39m=\u001b[39m _shard_group_bytes_to_disk(\n\u001b[0;32m    136\u001b[0m     write_dir, group_index, group_bytes, total_bytes, shard_size_bytes)\n\u001b[0;32m    138\u001b[0m weights_entries \u001b[39m=\u001b[39m _get_weights_manifest_for_group(group)\n\u001b[0;32m    139\u001b[0m manifest_entry \u001b[39m=\u001b[39m {\n\u001b[0;32m    140\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpaths\u001b[39m\u001b[39m'\u001b[39m: shard_filenames,\n\u001b[0;32m    141\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m'\u001b[39m: weights_entries\n\u001b[0;32m    142\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\sutyl\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflowjs\\write_weights.py:306\u001b[0m, in \u001b[0;36m_shard_group_bytes_to_disk\u001b[1;34m(write_dir, group_index, group_bytes, total_bytes, shard_size_bytes)\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[39m# Write the shard to disk.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m   \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(filepath, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 306\u001b[0m     f\u001b[39m.\u001b[39;49mwrite(shard)\n\u001b[0;32m    308\u001b[0m \u001b[39mreturn\u001b[39;00m filenames\n",
      "File \u001b[1;32mc:\\Users\\sutyl\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:99\u001b[0m, in \u001b[0;36mFileIO.write\u001b[1;34m(self, file_content)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, file_content):\n\u001b[0;32m     98\u001b[0m   \u001b[39m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prewrite_check()\n\u001b[0;32m    100\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writable_file\u001b[39m.\u001b[39mappend(compat\u001b[39m.\u001b[39mas_bytes(file_content))\n",
      "File \u001b[1;32mc:\\Users\\sutyl\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:84\u001b[0m, in \u001b[0;36mFileIO._prewrite_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_check_passed:\n\u001b[0;32m     82\u001b[0m   \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mPermissionDeniedError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     83\u001b[0m                                      \u001b[39m\"\u001b[39m\u001b[39mFile isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt open for writing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writable_file \u001b[39m=\u001b[39m _pywrap_file_io\u001b[39m.\u001b[39;49mWritableFile(\n\u001b[0;32m     85\u001b[0m     compat\u001b[39m.\u001b[39;49mpath_to_bytes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name), compat\u001b[39m.\u001b[39;49mas_bytes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__mode))\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a NewWriteableFile: model\\tfjs_model\\group1-shard1of4.bin : The system cannot find the file specified.\r\n; No such file or directory"
     ]
    }
   ],
   "source": [
    "# Convert tfjs model\n",
    "import tensorflowjs as tfjs\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model(\"model\\\\food_not_food_model_B0-V1.h5\")\n",
    "\n",
    "tfjs.converters.save_keras_model(model, \"model\\\\tfjs_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db9c5e734c6be09092103b64cdd5de4634025b6149e439930b56fa1ad2a2b58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
